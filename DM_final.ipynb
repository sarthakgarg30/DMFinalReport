{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee36427",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "!pip install xgboost\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install lightgbm\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4badce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import datetime\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import init_notebook_mode\n",
    "\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1cd2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_orig = pd.read_csv('/scratch/tmparule/train_data.csv')\n",
    "REPO_ID = \"AdithyaM-16/DMdataset\"\n",
    "FILENAME = \"train_data.csv\"\n",
    "\n",
    "train_df_orig = pd.read_csv(\n",
    "    hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = pd.read_csv('/scratch/tmparule/train_labels.csv')\n",
    "REPO_ID = \"AdithyaM-16/DMdataset\"\n",
    "FILENAME = \"train_labels.csv\"\n",
    "\n",
    "train_labels = pd.read_csv(\n",
    "    hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1394604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_read = train_df_orig.merge(train_labels, on='customer_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf8aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp=dict(layout=go.Layout(height=500, width=900))\n",
    "\n",
    "target=train_df_read.target.value_counts(normalize=True)\n",
    "target.rename(index={1:'Default',0:'Paid'},inplace=True)\n",
    "pal, color=['#016CC9','#DEB078'], ['#8DBAE2','#EDD3B3']\n",
    "fig=go.Figure()\n",
    "fig.add_trace(go.Pie(labels=target.index, values=target*100, hole=.35, showlegend=True,sort=False, marker=dict(colors=color,line=dict(color=pal,width=2.5))))\n",
    "# fig.update_layout(template=temp, title='Target Distribution', \n",
    "#                   legend=dict(traceorder='reversed',y=1.05,x=0),\n",
    "#                   uniformtext_minsize=15, uniformtext_mode='hide',width=700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting the statements per customer:\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(8, 5))\n",
    "train_sc = train_df_read.customer_ID.value_counts().value_counts().sort_index(ascending=False).rename('Train statements per customer')\n",
    "ax1.pie(train_sc, labels=train_sc.index)\n",
    "ax1.set_title(train_sc.name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of customer statments:\n",
    "plot_df=train_df_read.reset_index().groupby('S_2')['customer_ID'].nunique().reset_index()\n",
    "fig=go.Figure()\n",
    "fig.add_trace(go.Scatter(x=plot_df['S_2'], y=plot_df['customer_ID'], mode='lines',line=dict(color='skyblue', width=3)))\n",
    "fig.update_layout( title=\"Frequency of Customer Statements\", width=700,height=450,xaxis_title='Statement Date', yaxis_title='Number of Statements Issued')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency distribution\n",
    "\n",
    "delinquency_features = [c for c in train_df_read.columns if c.startswith('D_')]\n",
    "spend_features = [c for c in train_df_read.columns if c.startswith('S_')]\n",
    "payment_features = [c for c in train_df_read.columns if c.startswith('P_')]\n",
    "balance_features = [c for c in train_df_read.columns if c.startswith('B_')]\n",
    "risk_features = [c for c in train_df_read.columns if c.startswith('R_')]\n",
    "labels=['Delinquency', 'Spend','Payment','Balance','Risk']\n",
    "values= [len(delinquency_features), len(spend_features),len(payment_features), len(balance_features),len(risk_features)]\n",
    "\n",
    "\n",
    "## First Plot\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values,hole=.2)])\n",
    "\n",
    "fig.update_traces(marker=dict(colors=['#a43725', '#e0d5bd', '#beb29e', '#E6b6a4', '#c07156']))\n",
    "layout = dict(title = 'Feature Distribution',showlegend = True)\n",
    "fig[\"layout\"].update(layout)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735beb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of categorical variable:\n",
    "rgb=['rgba'+str(matplotlib.colors.to_rgba(i,0.7)) for i in pal]\n",
    "cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68', 'target']\n",
    "fig = make_subplots(rows=4, cols=3, subplot_titles=cat_cols[:-1], vertical_spacing=0.1)\n",
    "row=0\n",
    "c=[1,2,3]*5\n",
    "plot_df=train_df_read[cat_cols]\n",
    "for i,col in enumerate(cat_cols[:-1]):\n",
    "    if i%3==0:\n",
    "        row+=1\n",
    "    plot_df[col]=plot_df[col].astype(object)\n",
    "    df=plot_df.groupby(col)['target'].value_counts().rename('count').reset_index().replace('',np.nan)\n",
    "    \n",
    "    fig.add_trace(go.Bar(x=df[df.target==1][col], y=df[df.target==1]['count'],marker_color=rgb[1], marker_line=dict(color=pal[1],width=2),name='Default', showlegend=(True if i==0 else False)),row=row, col=c[i])\n",
    "    fig.add_trace(go.Bar(x=df[df.target==0][col], y=df[df.target==0]['count'],marker_color=rgb[0], marker_line=dict(color=pal[0],width=2),name='Paid', showlegend=(True if i==0 else False)),row=row, col=c[i])\n",
    "    if i%3==0:\n",
    "        fig.update_yaxes(title='Frequency',row=row,col=c[i])\n",
    "fig.update_layout(title=\"Distribution of Categorical Variables\",\n",
    "                  legend=dict(orientation=\"h\",yanchor=\"bottom\",y=1.03,xanchor=\"right\",x=0.2),\n",
    "                  barmode='group',height=1500,width=900)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18884ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test correlation with target:\n",
    "\n",
    "temp=dict(layout=go.Layout(height=500, width=1000))\n",
    "\n",
    "# Computes the pairwise Pearson correlation of all columns in the dataframe train.\n",
    "# 178 x 178, number of features = 190 but we excluded the categorical features\n",
    "\n",
    "train_df_2 = train_df_read.drop(['customer_ID', 'S_2', 'D_63', 'D_64'], axis = 1)\n",
    "corr=train_df_2.corr()\n",
    "# Filters the correlations to only those with the column Target,\n",
    "# then sorts them in descending order, and\n",
    "# excludes the correlation of Target with itself\n",
    "corr=corr['target'].sort_values(ascending=False)[1:]\n",
    "\n",
    "# Uses Seaborn to create a reversed red color palette with 135 different shades.\n",
    "pal=sns.color_palette(\"Reds_r\",135).as_hex()\n",
    "# Converts the hexadecimal colors in pal to RGB format (with added alpha transparency) using Matplotlib.\n",
    "rgb=['rgba'+str(matplotlib.colors.to_rgba(i,0.7)) for i in pal]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=corr[corr>=0],\n",
    "        y=corr[corr>=0].index, \n",
    "        marker_color=rgb,\n",
    "        orientation='h', \n",
    "        marker_line=dict(color=pal,width=2), \n",
    "        name='',\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "pal=sns.color_palette(\"Blues\",100).as_hex()\n",
    "rgb=['rgba'+str(matplotlib.colors.to_rgba(i,0.7)) for i in pal]\n",
    "\n",
    "# Adds the positively correlated features as horizontal bars to the figure using the colors defined earlier.\n",
    "# Sets the orientation of the bars to horizontal, specifies hover information, \n",
    "# and ensures that these bars won't appear in a legend.\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=corr[corr<0], \n",
    "        y=corr[corr<0].index, \n",
    "        marker_color=rgb[25:], \n",
    "        orientation='h', \n",
    "        marker_line=dict(color=pal[25:],width=2), \n",
    "        name='',\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template=temp,\n",
    "    title=\"Feature Correlations with Target\",\n",
    "    xaxis_title=\"Correlation\", \n",
    "    margin=dict(l=150),\n",
    "    height=3000, \n",
    "    width=700, \n",
    "    hovermode='closest' #ensures that the closest data point to the hover position will be shown in the hover label.\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_read.sort_values(by='S_2', inplace=True)\n",
    "train_df = train_df_read.groupby('customer_ID').tail(1).set_index('customer_ID', drop=True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9484477",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['S_2', 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "cat_cols = [col for col in categorical_cols if col in train_df.columns]\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "# categorical_cols.remove('D_66')\n",
    "\n",
    "train_df[cat_cols] = enc.fit_transform(train_df[cat_cols])\n",
    "#[cat_cols] = enc.transform(test_df[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df_ini = train_df.drop(['customer_ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e45dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_ini = train_df\n",
    "median_values_train = train_df.median()\n",
    "train_df_ini = train_df.fillna(median_values_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_initial = train_df.drop['']\n",
    "feature_list = [col for col in train_df_ini if col not in ['target']]\n",
    "\n",
    "X = train_df_ini[feature_list]\n",
    "y = train_df_ini['target']\n",
    "\n",
    "print(\"X shape \", X.shape)\n",
    "print(\"y shape \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"X_train shape is = \",x_train.shape)\n",
    "print(f\"Y_train shape is = \",y_train.shape)\n",
    "print(f\"X_test shape is = \",x_test.shape)\n",
    "print(f\"Y_test shape is = \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c41b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=42)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "logreg_model.fit(x_train, y_train)\n",
    "y_pred = logreg_model.predict(x_test)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Calculate the time taken\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Print the time taken\n",
    "print(\"Time taken(ms):\", time_taken.total_seconds()*1000)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1score = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-score: {f1score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d018086",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "rf_classifier.fit(x_train, y_train)\n",
    "y_pred = rf_classifier.predict(x_test)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "time_taken = end_time - start_time\n",
    "print(\"Time taken(ms):\", time_taken.total_seconds()*1000)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1score = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-score: {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Train XGBoost model\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',  # Specify the learning task and the corresponding objective\n",
    "    max_depth=3,  # Maximum depth of a tree\n",
    "    learning_rate=0.1,  # Learning rate\n",
    "    n_estimators=10  # Number of boosting rounds\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred_xg = model.predict(x_test)\n",
    "\n",
    "# end_time = datetime.datetime.now()\n",
    "# time_taken = end_time - start_time\n",
    "# print(\"Time taken(ms):\", time_taken.total_seconds()*1000)\n",
    "\n",
    "accuracy_xg = accuracy_score(y_test, y_pred_xg)\n",
    "precision_xg = precision_score(y_test, y_pred_xg)\n",
    "recall_xg = recall_score(y_test, y_pred_xg)\n",
    "f1score_xg = f1_score(y_test, y_pred_xg)\n",
    "conf_matrix_xg = confusion_matrix(y_test, y_pred_xg)\n",
    "# classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy_xg}')\n",
    "print(f'Precision: {precision_xg}')\n",
    "print(f'Recall: {recall_xg}')\n",
    "print(f'F1-score: {f1score_xg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26921bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_Val = np.array(train_df.isnull().sum())\n",
    "NaN_prec = np.array((train_df.isnull().sum() * 100 / len(train_df)).round(2))\n",
    "NaN_Col = pd.DataFrame([np.array(list(train_df.columns)).T,NaN_Val.T,NaN_prec.T,np.array(list(train_df_read.dtypes)).T], index=['Features','Num of Missing values','Percentage','DataType']\n",
    ").transpose()\n",
    "# pd.set_option('display.max_rows',Â None)\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "NaN_Col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_threshold = 80\n",
    "\n",
    "nan_cols = NaN_Col[NaN_Col['Percentage']>null_threshold]['Features'].to_list()\n",
    "# nan_cols\n",
    "train_df_2 = train_df.drop(nan_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a754b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bfce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_values_train = train_df_2.median()\n",
    "train_df_2.fillna(median_values_train, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df_2.isnull().sum().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_without_target_df = train_df_2.drop(['target'], axis = 1)\n",
    "\n",
    "corr_matrix = train_without_target_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48bc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_limit = 0.9\n",
    "\n",
    "col_high_corr = set()\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if(corr_matrix.iloc[i, j] > corr_limit):\n",
    "            colname = corr_matrix.columns[i]\n",
    "            col_high_corr.add(colname)\n",
    "            \n",
    "col_high_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0840d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = train_df_2.drop(col_high_corr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10803e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af2c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [col for col in train_df_2 if col not in ['target']]\n",
    "\n",
    "X = train_df_2[feature_list]\n",
    "y = train_df_2['target']\n",
    "\n",
    "print(\"X shape \", X.shape)\n",
    "print(\"y shape \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a7533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"X_train shape is = \",x_train.shape)\n",
    "print(f\"Y_train shape is = \",y_train.shape)\n",
    "print(f\"X_test shape is = \",x_test.shape)\n",
    "print(f\"Y_test shape is = \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=42)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "logreg_model.fit(x_train, y_train)\n",
    "y_pred_log = logreg_model.predict(x_test)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Calculate the time taken\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Print the time taken\n",
    "print(\"Time taken(ms):\", time_taken.total_seconds()*1000)\n",
    "\n",
    "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
    "precision_log = precision_score(y_test, y_pred_log)\n",
    "recall_log = recall_score(y_test, y_pred_log)\n",
    "f1score_log = f1_score(y_test, y_pred_log)\n",
    "conf_matrix_log = confusion_matrix(y_test, y_pred_log)\n",
    "# classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy_log}')\n",
    "print(f'Precision: {precision_log}')\n",
    "print(f'Recall: {recall_log}')\n",
    "print(f'F1-score: {f1score_log}')\n",
    "# print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "# print(f'Classification Report:\\n{classification_rep}')# Plot confusion matrix as heatmap\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(conf_matrix_log, annot=True, cmap='Blues', fmt='g', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Predict probabilities instead of labels\n",
    "y_pred_proba = logreg_model.predict_proba(x_test)[:, 1]  # Predict probabilities for class 1\n",
    "\n",
    "# Calculate false positive rate, true positive rate, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Calculate area under the curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "rf_classifier.fit(x_train, y_train)\n",
    "y_pred_rf = rf_classifier.predict(x_test)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "time_taken = end_time - start_time\n",
    "print(\"Time taken(ms):\", time_taken.total_seconds()*1000)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1score_rf = f1_score(y_test, y_pred_rf)\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "# classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy_rf}')\n",
    "print(f'Precision: {precision_rf}')\n",
    "print(f'Recall: {recall_rf}')\n",
    "print(f'F1-score: {f1score_rf}')\n",
    "# print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "# print(f'Classification Report:\\n{classification_rep}')# Plot confusion matrix as heatmap\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(conf_matrix_rf, annot=True, cmap='Blues', fmt='g', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Predict probabilities instead of labels\n",
    "y_pred_proba_rf = rf_classifier.predict_proba(x_test)[:, 1]  # Predict probabilities for class 1\n",
    "\n",
    "# Calculate false positive rate, true positive rate, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_rf)\n",
    "\n",
    "# Calculate area under the curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e04315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Train XGBoost model\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',  # Specify the learning task and the corresponding objective\n",
    "    max_depth=3,  # Maximum depth of a tree\n",
    "    learning_rate=0.1,  # Learning rate\n",
    "    n_estimators=10  # Number of boosting rounds\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred_xg = model.predict(x_test)\n",
    "\n",
    "# end_time = datetime.datetime.now()\n",
    "# time_taken = end_time - start_time\n",
    "# print(\"Time taken(ms):\", time_taken.total_seconds()*1000)\n",
    "\n",
    "accuracy_xg = accuracy_score(y_test, y_pred_xg)\n",
    "precision_xg = precision_score(y_test, y_pred_xg)\n",
    "recall_xg = recall_score(y_test, y_pred_xg)\n",
    "f1score_xg = f1_score(y_test, y_pred_xg)\n",
    "conf_matrix_xg = confusion_matrix(y_test, y_pred_xg)\n",
    "# classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy_xg}')\n",
    "print(f'Precision: {precision_xg}')\n",
    "print(f'Recall: {recall_xg}')\n",
    "print(f'F1-score: {f1score_xg}')\n",
    "# print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "# print(f'Classification Report:\\n{classification_rep}')# Plot confusion matrix as heatmap\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(conf_matrix_xg, annot=True, cmap='Blues', fmt='g', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Obtain predicted probabilities for the positive class\n",
    "y_pred_proba_xg = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Calculate false positive rate, true positive rate, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_xg)\n",
    "\n",
    "# Calculate area under the curve (AUC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26046395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train.to_numpy(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test.to_numpy(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the neural decision tree model\n",
    "class NeuralDecisionTree(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(NeuralDecisionTree, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = x_train.shape[1]  # Assuming x_train is a 2D array\n",
    "num_classes = len(torch.unique(y_train_tensor))\n",
    "model = NeuralDecisionTree(input_dim, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_pred_probs = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        y_pred_probs.extend(outputs.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "y_pred_probs = torch.tensor(y_pred_probs)\n",
    "y_true = torch.tensor(y_true)\n",
    "\n",
    "_, y_pred = torch.max(y_pred_probs, 1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "# roc_auc = roc_auc_score(y_true, y_pred_probs, average='weighted', multi_class='ovr')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "# print(f\"ROC-AUC Score: {roc_auc}\")\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(conf_matrix_ndt, annot=True, cmap='Blues', fmt='g', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Assuming pred_values contains predicted probabilities or scores, and expected_values contains true labels\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols=['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_68']\n",
    "\n",
    "d_train = lgb.Dataset(x_train, label=y_train, categorical_feature = categorical_cols)\n",
    "\n",
    "params = {'objective': 'binary','n_estimators': 1200,'metric': 'binary_logloss','boosting': 'gbdt','num_leaves': 90,'reg_lambda' : 50,'colsample_bytree': 0.19,'learning_rate': 0.03,'min_child_samples': 2400,'max_bins': 511,'seed': 42,'verbose': -1}\n",
    "\n",
    "# trained model with 100 iterations\n",
    "model = lgb.train(params, d_train, 100)\n",
    "\n",
    "y_pred_prob_gm = model.predict(x_test)\n",
    "#Map probablities to 0 or 1\n",
    "y_pred_gbm = [1 if x >= 0.5 else 0 for x in y_pred_prob_gm]\n",
    "\n",
    "accuracy_gbm = accuracy_score(y_test, y_pred_gbm)\n",
    "precision_gbm = precision_score(y_test, y_pred_gbm)\n",
    "recall_gbm = recall_score(y_test, y_pred_gbm)\n",
    "f1score_gbm = f1_score(y_test, y_pred_gbm)\n",
    "conf_matrix_gbm = confusion_matrix(y_test, y_pred_gbm)\n",
    "# classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy_gbm}')\n",
    "print(f'Precision: {precision_gbm}')\n",
    "print(f'Recall: {recall_gbm}')\n",
    "print(f'F1-score: {f1score_gbm}')\n",
    "# print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "# print(f'Classification Report:\\n{classification_rep}')# Plot confusion matrix as heatmap\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(conf_matrix_gbm, annot=True, cmap='Blues', fmt='g', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Assuming pred_values contains predicted probabilities or scores, and expected_values contains true labels\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_gm)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824baf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X\n",
    "y_train = y\n",
    "# Convert data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train.to_numpy(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "# x_test_tensor = torch.tensor(x_test.to_numpy(), dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "# Define the neural decision tree model\n",
    "class NeuralDecisionTree(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(NeuralDecisionTree, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = x_train.shape[1]  # Assuming x_train is a 2D array\n",
    "num_classes = len(torch.unique(y_train_tensor))\n",
    "model = NeuralDecisionTree(input_dim, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define number of folds\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "fold_accuracy = []\n",
    "fold_precision = []\n",
    "fold_recall = []\n",
    "fold_f1 = []\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for fold, (train_indices, test_indices) in enumerate(kf.split(x_train_tensor)):\n",
    "    print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    x_train_fold = x_train_tensor[train_indices]\n",
    "    y_train_fold = y_train_tensor[train_indices]\n",
    "    x_val_fold = x_train_tensor[test_indices]\n",
    "    y_val_fold = y_train_tensor[test_indices]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(x_train_fold, y_train_fold)\n",
    "    val_dataset = TensorDataset(x_val_fold, y_val_fold)\n",
    "\n",
    "    # Create data loaders\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    # Append metrics to lists\n",
    "    fold_accuracy.append(accuracy)\n",
    "    fold_precision.append(precision)\n",
    "    fold_recall.append(recall)\n",
    "    fold_f1.append(f1)\n",
    "\n",
    "    print(f\"Accuracy on validation set: {accuracy}\")\n",
    "    print(f\"Precision on validation set: {precision}\")\n",
    "    print(f\"Recall on validation set: {recall}\")\n",
    "    print(f\"F1 score on validation set: {f1}\")\n",
    "\n",
    "# Calculate average metrics over all folds\n",
    "average_accuracy = sum(fold_accuracy) / num_folds\n",
    "average_precision = sum(fold_precision) / num_folds\n",
    "average_recall = sum(fold_recall) / num_folds\n",
    "average_f1 = sum(fold_f1) / num_folds\n",
    "\n",
    "print(f\"Average accuracy over {num_folds} folds: {average_accuracy}\")\n",
    "print(f\"Average precision over {num_folds} folds: {average_precision}\")\n",
    "print(f\"Average recall over {num_folds} folds: {average_recall}\")\n",
    "print(f\"Average F1 score over {num_folds} folds: {average_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8fceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_68']\n",
    "# x_train = x_train.drop(['Customer_ID'])\n",
    "# Define LightGBM dataset\n",
    "# d_train = lgb.Dataset(x_train, label=y_train, categorical_feature=categorical_cols)\n",
    "x_train_lg = X\n",
    "y_train_lg = y\n",
    "y_train_lg.head()\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'n_estimators': 1200,\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 90,\n",
    "    'reg_lambda': 50,\n",
    "    'colsample_bytree': 0.19,\n",
    "    'learning_rate': 0.03,\n",
    "    'min_child_samples': 2400,\n",
    "    'max_bins': 511,\n",
    "    'seed': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "\n",
    "fold_accuracy = []\n",
    "fold_precision = []\n",
    "fold_recall = []\n",
    "fold_f1 = []\n",
    "\n",
    "# for fold, (train_index, val_index) in enumerate(kf.split(x_train)):\n",
    "#     print(fold, type(train_index), type(val_index))\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(x_train_lg)):\n",
    "    print(f\"Fold {fold+1}/{num_folds}\")\n",
    "    print(train_index, val_index)\n",
    "    # Split data into train and validation sets\n",
    "    x_train_fold, x_val_fold = x_train_lg.iloc[train_index], x_train_lg.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_lg.iloc[train_index], y_train_lg.iloc[val_index]\n",
    "\n",
    "    # Define LightGBM dataset for the fold\n",
    "    d_train_fold = lgb.Dataset(x_train_fold, label=y_train_fold, categorical_feature=categorical_cols)\n",
    "    d_val_fold = lgb.Dataset(x_val_fold, label=y_val_fold, reference=d_train_fold)\n",
    "\n",
    "    # Train the model\n",
    "    model = lgb.train(params, d_train_fold, valid_sets=[d_val_fold])\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred_proba = model.predict(x_val_fold)\n",
    "\n",
    "    # Round probabilities to get binary predictions\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Calculate evaluation metrics for the fold\n",
    "    accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "    precision = precision_score(y_val_fold, y_pred)\n",
    "    recall = recall_score(y_val_fold, y_pred)\n",
    "    f1 = f1_score(y_val_fold, y_pred)\n",
    "\n",
    "    # Append metrics to lists\n",
    "    fold_accuracy.append(accuracy)\n",
    "    fold_precision.append(precision)\n",
    "    fold_recall.append(recall)\n",
    "    fold_f1.append(f1)\n",
    "\n",
    "    print(f\"Accuracy on validation set: {accuracy}\")\n",
    "    print(f\"Precision on validation set: {precision}\")\n",
    "    print(f\"Recall on validation set: {recall}\")\n",
    "    print(f\"F1 score on validation set: {f1}\")\n",
    "\n",
    "# Calculate average metrics over all folds\n",
    "average_accuracy = sum(fold_accuracy) / num_folds\n",
    "average_precision = sum(fold_precision) / num_folds\n",
    "average_recall = sum(fold_recall) / num_folds\n",
    "average_f1 = sum(fold_f1) / num_folds\n",
    "\n",
    "print(f\"Average accuracy over {num_folds} folds: {average_accuracy}\")\n",
    "print(f\"Average precision over {num_folds} folds: {average_precision}\")\n",
    "print(f\"Average recall over {num_folds} folds: {average_recall}\")\n",
    "print(f\"Average F1 score over {num_folds} folds: {average_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bea95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75bb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08a7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfab6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea9296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a5a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9944a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b7fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488652cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
